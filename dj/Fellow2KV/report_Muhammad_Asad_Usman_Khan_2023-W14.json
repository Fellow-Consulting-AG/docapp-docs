{
    "author": "Muhammad Asad Usman Khan",
    "week": "2023-W14",
    "total_commits": 12,
    "author_image": "https://avatars.githubusercontent.com/u/83513548?v=4",
    "commits": [
        {
            "sha": "108fd7e9b9417b61f40997ab7dbab45ce922e89f",
            "message": "fd",
            "code_changes": "@@ -59,7 +59,6 @@ networkx~=2.7\n nltk==3.7\n numpy==1.24.2\n oauthlib==3.1.1\n-openai==0.27.4\n opencv-python~=4.5\n openpyxl~=3.0\n packaging~=21.3"
        },
        {
            "sha": "1a45eef5758cf7d8d955d095fbe56abc4b0850f7",
            "message": "fd",
            "code_changes": "@@ -177,6 +177,7 @@ def transform_ai_response(ai_resp_json):\n                                 \"normalizedY0\": nv0[\"y\"],\n                                 \"normalizedX1\": nv2[\"x\"],\n                                 \"normalizedY1\": nv2[\"y\"],\n+                                \"mergeNextToken\": len(hocr_content[startIndex:endIndex]) == len(text),\n                                 # \"normalizedVertices\": item[\"layout\"][\"boundingPoly\"][\n                                 #     \"normalizedVertices\"\n                                 # ],"
        },
        {
            "sha": "e3cf3c75fa1e1795be700bb8d4957d871294c033",
            "message": "fd",
            "code_changes": "@@ -47,7 +47,8 @@ def post(self):\n             self: write your description\n         \"\"\"\n         try:\n-            import openai\n+            #import openai\n+            pass\n         except Exception as ex:\n             return {\n                 \"success\": False,"
        },
        {
            "sha": "8003956974ac99f5f8d80338d1874a69c4fa69f7",
            "message": "fd",
            "code_changes": "@@ -24,6 +24,39 @@\n namespace = Namespace(\"Table Extract V3\", description=\"Table Extract Endpoints\")\n doc2_base_directory = \"documents\"\n \n+get_auto_table_parser = api.parser()\n+\n+\n+@namespace.route(\"/doc2/test_openai\", methods=[\"POST\"])\n+@namespace.doc(\n+    params={\n+        \"Authorization\": {\n+            \"in\": \"header\",\n+            \"description\": \"Authorization: Bearer <access_token>\",\n+        }\n+    }\n+)\n+class TestOpenAI(Resource):\n+    @namespace.expect(get_auto_table_parser)\n+    @multi_auth.login_required\n+    @namespace.doc(\"Gets table from the co-ordinates\")\n+    def post(self):\n+        \"\"\"\n+        Extracts tables from OCR document.\n+        Args:\n+            self: write your description\n+        \"\"\"\n+        try:\n+            import openai\n+        except Exception as ex:\n+            return {\n+                \"success\": False,\n+                \"message\": str(ex),\n+                \"error_message\": repr(ex) + \"\\n\" + traceback.format_exc(),\n+            }\n+        return {\n+                \"success\": True,\n+            } \n \n get_auto_table_parser = api.parser()\n get_auto_table_parser.add_argument("
        },
        {
            "sha": "5511844fb6430d440902872c12f36ca68ab78129",
            "message": "Merge pull request #857 from Fellow-Consulting-AG/stage\n\nStage",
            "code_changes": "@@ -111,6 +111,7 @@ class OCR_SERVICE_TYPES:\n     EASYOCR = \"EASYOCR\"\n     TESSERACT = \"TESSERACT\"\n     GOOGLE = \"GOOGLE\"\n+    AI = \"AI\"\n     PDF_MINER = \"PDF_MINER\"\n     E_TEXT = \"E_TEXT\"\n "
        },
        {
            "sha": "6771f31ea997e40520f78a69853f6e88cd8d54bd",
            "message": "Merge pull request #856 from Fellow-Consulting-AG/dev\n\nfd",
            "code_changes": "@@ -111,6 +111,7 @@ class OCR_SERVICE_TYPES:\n     EASYOCR = \"EASYOCR\"\n     TESSERACT = \"TESSERACT\"\n     GOOGLE = \"GOOGLE\"\n+    AI = \"AI\"\n     PDF_MINER = \"PDF_MINER\"\n     E_TEXT = \"E_TEXT\"\n "
        },
        {
            "sha": "adf55591149fa06b6cd07e505fdcfb2e66b47bb5",
            "message": "Delete google_ocr.py",
            "code_changes": "@@ -1,278 +0,0 @@\n-import json\n-import logging\n-import tempfile\n-import time\n-from base64 import decodebytes\n-\n-import constants\n-import helper.ai.ai_common as ai\n-from helper.util import find_related_locale\n-from logger import get_logger\n-from models import OCRConfigurations\n-from util import sync_wrapper\n-\n-from .iocr import IOCRService\n-\n-logger = get_logger(\"google_ocr\")\n-\n-url = \"https://eu-documentai.googleapis.com/v1/projects/342434080717/locations/eu/processors/4bfbf986c18da072:process\"\n-\n-\n-class GOOGLE_OCR(IOCRService):\n-    def __init__(self, config: OCRConfigurations, **kwargs):\n-        self.config: OCRConfigurations = config\n-        pass\n-\n-    def __get_ocr_data__(self, source_file_path, doc_id=None):\n-        ai_resp_json = ai.get_ai_response_v3(\n-            constants.AI_REQUEST_TYPE.OCR, url, source_file_path, None, doc_id\n-        )\n-        document = GOOGLE_OCR.transform_ai_response(ai_resp_json)\n-\n-        document[\"doc_lang\"] = \"de\"\n-        document[\"doc_origin\"] = \"DE\"\n-        document[\"doc_locale\"] = \"de_DE\"\n-        document[\"ocr_lang\"] = \"de\"\n-        document[\"ocr_processor\"] = \"ai-ocr-service\"\n-\n-        GOOGLE_OCR.populate_origin_and_locale(ai_resp_json, document)\n-\n-        return document, ai_resp_json\n-\n-    def get_ocr_data(self, user, source_file_path):\n-        document, _ = self.__get_ocr_data__(source_file_path, None)\n-        return document\n-\n-    @sync_wrapper(module=\"ocr\")\n-    def extract_ocr_data_and_images(\n-        self, storage_manager, source_file_path, destination_dir, doc_id, user\n-    ):\n-        document, ai_resp_json = self.__get_ocr_data__(source_file_path, None)\n-\n-        document[\"doc_id\"] = doc_id\n-        document[\"ocr_processor\"] = \"ai-ocr-service\"\n-\n-        logger.info(f\"Going to save extracted image and ocr document\")\n-        start_time = time.perf_counter()\n-        # extracting and saving images\n-        images_paths, quality = GOOGLE_OCR.extract_and_save_images(\n-            ai_resp_json, doc_id, storage_manager, destination_dir\n-        )\n-        document[\"images\"] = images_paths\n-        document[\"qualitycheck\"] = quality\n-        end_time = time.perf_counter()\n-        logger.customer(\"Document successfully OCRed with AI\")\n-        logger.customer(f\"Saved document and images.\")\n-        return document\n-\n-    def transform_ai_response(ai_resp_json):\n-        # document = {\"text\": \"\"}\n-        document = {}\n-        if not ai_resp_json:\n-            return document\n-\n-        pages = []\n-\n-        indexIncrement = 0\n-        pageNumberIncrement = 0\n-        for resp_json in ai_resp_json:\n-            hocr_content = resp_json.get(\"document\", {}).get(\"text\", \"\")\n-            pageStartIndex = 0\n-            pageEndIndex = 0\n-            for index, page in enumerate(\n-                resp_json.get(\"document\", {}).get(\"pages\", [])\n-            ):\n-                if (\n-                    not \"width\" in page.get(\"dimension\", {})\n-                    and not \"width\" in page.get(\"image\", {})\n-                ) or (\n-                    not \"height\" in page.get(\"dimension\", {})\n-                    and not \"height\" in page.get(\"image\", {})\n-                ):\n-                    continue\n-                width = page[\"dimension\"].get(\"width\", page[\"image\"][\"width\"])\n-                height = page[\"dimension\"].get(\"height\", page[\"image\"][\"height\"])\n-\n-                first_text_segment = page[\"layout\"][\"textAnchor\"][\"textSegments\"][0]\n-\n-                if \"startIndex\" in first_text_segment:\n-                    pageStartIndex = int(first_text_segment[\"startIndex\"])\n-                elif index > 0:\n-                    pass\n-                    # previous_page = resp_json.get(\"document\",{})[\"pages\"][index-1]\n-                    # previous_page_first_text_segment = previous_page[\"layout\"][\"textAnchor\"][\"textSegments\"][0]\n-                    # pageStartIndex = int(pages[len(pages)-1]['endIndex'])\n-\n-                if \"endIndex\" in first_text_segment:\n-                    pageEndIndex = int(first_text_segment[\"endIndex\"])\n-\n-                current_page = {\n-                    \"startIndex\": pageStartIndex + indexIncrement,\n-                    \"endIndex\": pageEndIndex + indexIncrement,\n-                    \"pageText\": hocr_content[pageStartIndex:pageEndIndex],\n-                    \"pageNumber\": page[\"pageNumber\"] + pageNumberIncrement,\n-                    \"width\": width,\n-                    \"height\": height,\n-                    # \"indexIncrement\": indexIncrement,\n-                }\n-                # sections = [\"blocks\", \"paragraphs\", \"lines\", \"tokens\"]\n-                sections = [\"tokens\"]\n-                for section in sections:\n-                    adjustment_factor = 0\n-                    items = []\n-                    if section not in page:\n-                        current_page[section] = items\n-                        continue\n-                    for item in page[section]:\n-                        textSegment = item[\"layout\"][\"textAnchor\"][\"textSegments\"][0]\n-                        startIndex = 0\n-                        if \"startIndex\" in textSegment:\n-                            startIndex = int(textSegment[\"startIndex\"])\n-\n-                        endIndex = 0\n-                        if \"endIndex\" in textSegment:\n-                            endIndex = int(textSegment[\"endIndex\"])\n-\n-                        startIndex = startIndex + adjustment_factor\n-                        endIndex = endIndex + adjustment_factor\n-\n-                        if (\n-                            section == \"tokens\"\n-                            and \"\\n\" in hocr_content[startIndex:endIndex].rstrip()\n-                        ):\n-                            adjustment_factor = adjustment_factor + 1\n-                            startIndex += 1\n-                            endIndex += 1\n-\n-                        text = hocr_content[startIndex:endIndex].strip()\n-                        nv0 = item[\"layout\"][\"boundingPoly\"][\"normalizedVertices\"][0]\n-                        nv2 = item[\"layout\"][\"boundingPoly\"][\"normalizedVertices\"][2]\n-                        if (\n-                            \"x\" not in nv0\n-                            or \"x\" not in nv2\n-                            or \"y\" not in nv0\n-                            or \"y\" not in nv2\n-                            or nv2[\"x\"] < 0\n-                            or nv0[\"x\"] < 0\n-                            or nv2[\"y\"] < 0\n-                            or nv0[\"y\"] < 0\n-                            or nv2[\"x\"] > 1\n-                            or nv0[\"x\"] > 1\n-                            or nv2[\"y\"] > 1\n-                            or nv0[\"y\"] > 1\n-                            or nv2[\"x\"] < nv0[\"x\"]\n-                            or nv2[\"y\"]\n-                            < nv0[\"y\"]  # to remove garbage for table extraction\n-                        ):\n-                            continue\n-\n-                        confidence = round(item[\"layout\"].get(\"confidence\", 1) * 100)\n-                        items.append(\n-                            {\n-                                \"text\": text,\n-                                \"startIndex\": startIndex + indexIncrement,\n-                                \"endIndex\": endIndex + indexIncrement,\n-                                \"confidence\": confidence,\n-                                \"x0\": round(nv0[\"x\"] * width),\n-                                \"y0\": round(nv0[\"y\"] * height),\n-                                \"x1\": round(nv2[\"x\"] * width),\n-                                \"y1\": round(nv2[\"y\"] * height),\n-                                \"normalizedX0\": nv0[\"x\"],\n-                                \"normalizedY0\": nv0[\"y\"],\n-                                \"normalizedX1\": nv2[\"x\"],\n-                                \"normalizedY1\": nv2[\"y\"],\n-                                # \"normalizedVertices\": item[\"layout\"][\"boundingPoly\"][\n-                                #     \"normalizedVertices\"\n-                                # ],\n-                            }\n-                        )\n-\n-                    current_page[section] = items\n-\n-                pages.append(current_page)\n-\n-                if page[\"pageNumber\"] == 10:\n-                    indexIncrement = indexIncrement + pageEndIndex\n-                    pageNumberIncrement = pageNumberIncrement + 10\n-                    pageStartIndex = 0\n-                    pageEndIndex = 0\n-\n-            # document[\"text\"] += hocr_content\n-        document[\"pages\"] = pages\n-\n-        return document\n-\n-    def get_detailed_images(ai_resp_json):\n-        images = []\n-        page_number = 1\n-        for resp_json in ai_resp_json:\n-            document = resp_json.get(\"document\", {})\n-            ai_pages = document.get(\"pages\", [])\n-\n-            for ai_page in ai_pages:\n-                image = ai_page[\"image\"]\n-                image[\"pageNumber\"] = page_number\n-                images.append(image)\n-                page_number += 1\n-\n-        return images\n-\n-    def extract_and_save_images(ai_resp_json, doc_id, storage_manager, dest_dir):\n-        images_paths = []\n-        doc_quality = []\n-        counter = 0\n-        ai_images = GOOGLE_OCR.get_detailed_images(ai_resp_json)\n-\n-        image_dest_path = dest_dir + \"/images\"\n-\n-        for ai_image in ai_images:\n-            page = str(ai_image[\"pageNumber\"])\n-            img_data = decodebytes(ai_image[\"content\"].encode())\n-            storage_manager.save_file_data(img_data, f\"{page}.png\", image_dest_path)\n-            img_data = {\n-                \"path\": f\"doc2/images/{doc_id}/{page}.png\",\n-                \"filename\": f\"{page}.png\",\n-                \"pageNumber\": ai_image[\"pageNumber\"],\n-                \"height\": ai_image[\"height\"],\n-                \"width\": ai_image[\"width\"],\n-            }\n-\n-            if \"entities\" in ai_resp_json[0][\"document\"]:\n-                for entity in ai_resp_json[0][\"document\"][\"entities\"]:\n-                    conf_percent = entity[\"confidence\"]\n-                    doc_quality.append(\n-                        {\"page\": page, \"overall\": f\"{conf_percent*100}%\"}\n-                    )\n-\n-                    for prop in entity[\"properties\"]:\n-                        if \"confidence\" in prop:\n-                            conf_percent = prop[\"confidence\"]\n-                            doc_quality[counter][prop[\"type\"]] = f\"{conf_percent*100}%\"\n-                    break\n-                counter += 1\n-\n-            images_paths.append(img_data)\n-        return images_paths, doc_quality\n-\n-    def save_ocr_content(document, doc_id, storage_manager, dest_dir):\n-        with tempfile.NamedTemporaryFile(\n-            suffix=\".json\", mode=\"w\", encoding=\"utf8\"\n-        ) as outfile:\n-            outfile.write(json.dumps(document) + \"\\n\")\n-            outfile.seek(0)\n-            storage_manager.save_file(\n-                outfile.name, doc_id + \"_ai_transform.json\", dest_dir\n-            )\n-\n-    def populate_origin_and_locale(ai_resp, document):\n-        ai_resp_document = ai_resp[0][\"document\"]\n-        first_page = ai_resp_document[\"pages\"][0]\n-\n-        document[\"doc_lang\"] = \"de\"\n-        if len(first_page.get(\"detectedLanguages\", [])) > 0:\n-            document[\"doc_lang\"] = first_page.get(\"detectedLanguages\")[0].get(\n-                \"languageCode\"\n-            )\n-        document[\"ocr_lang\"] = document[\"doc_lang\"]\n-        doc_locale = document.get(\"doc_lang\") + \"_\" + document.get(\"doc_origin\")\n-        document[\"doc_locale\"] = find_related_locale(doc_locale, prefer_lang=True)"
        },
        {
            "sha": "541e0000983484cd3b5bf9a592c930d9f38a8ea2",
            "message": "fd",
            "code_changes": "@@ -4,10 +4,8 @@\n \n from shapely.geometry import Polygon\n \n-from helper.form_extractor.fill_ratio import (\n-    get_field_fill_ratio,\n-    get_opencv_image_from_url,\n-)\n+from helper.form_extractor.fill_ratio import (get_field_fill_ratio,\n+                                              get_opencv_image_from_url)\n from helper.util import clean_amount_fields, clean_string\n \n sys.path.append(\".\")\n@@ -17,9 +15,9 @@\n \n import fellow2kv.config as config\n from fellow2kv.extension import storagemanager_doc2\n-from helper.common_regex import CURRENCY_REGEX, NUMERIC_AND_AMOUNT_SEPARATORS_REGEX\n+from helper.common_regex import (CURRENCY_REGEX,\n+                                 NUMERIC_AND_AMOUNT_SEPARATORS_REGEX)\n from helper.extract_util_errors import InvalidDirectionError, UndifiendError\n-\n # from fellow2kv.extension storagemanager_doc2\n from logger import get_logger\n \n@@ -884,7 +882,7 @@ def custom_lines_within_bounds(self, custom_lines, coordinates):\n         new_custom_lines = []\n         for line in custom_lines:\n             if line[\"y1\"] > coordinates[\"y0\"]:\n-                if not new_custom_lines and line[\"normalizedX0\"] > 0.5:\n+                if not new_custom_lines and line[\"normalizedX0\"] > 0.25:\n                     continue\n                     # table headers dont start from middle of the pagel so avoid making bad patterns.\n                 coords = None"
        },
        {
            "sha": "487f22a407148e95717f9f0c381df6ef86c1cf51",
            "message": "fd",
            "code_changes": "@@ -1346,8 +1346,7 @@ def get_table_rules_for_export_v3(extractor, table):\n \n         if pageNumber == 1:\n             is_first_page = True\n-\n-        if pageNumber < len(document.get(\"pages\")):\n+        elif pageNumber < len(document.get(\"pages\")):\n             is_middle_page = True\n \n         width = page.get(\"width\", 0)"
        },
        {
            "sha": "6126217dcb0e31042d67165af29fd946432f4c68",
            "message": "Merge pull request #855 from Fellow-Consulting-AG/stage\n\nStage",
            "code_changes": "@@ -76,14 +76,18 @@ def transform_ai_response(ai_resp_json):\n         indexIncrement = 0\n         pageNumberIncrement = 0\n         for resp_json in ai_resp_json:\n-            hocr_content = resp_json.get(\"document\",{}).get(\"text\", \"\")\n+            hocr_content = resp_json.get(\"document\", {}).get(\"text\", \"\")\n             pageStartIndex = 0\n             pageEndIndex = 0\n-            for index, page in enumerate(resp_json.get(\"document\",{}).get(\"pages\", [])):\n+            for index, page in enumerate(\n+                resp_json.get(\"document\", {}).get(\"pages\", [])\n+            ):\n                 if (\n-                    not \"width\" in page.get(\"dimension\", {}) and not \"width\" in page.get(\"image\", {})\n+                    not \"width\" in page.get(\"dimension\", {})\n+                    and not \"width\" in page.get(\"image\", {})\n                 ) or (\n-                    not \"height\" in page.get(\"dimension\", {}) and not \"height\" in page.get(\"image\", {})\n+                    not \"height\" in page.get(\"dimension\", {})\n+                    and not \"height\" in page.get(\"image\", {})\n                 ):\n                     continue\n                 width = page[\"dimension\"].get(\"width\", page[\"image\"][\"width\"])\n@@ -202,7 +206,7 @@ def get_detailed_images(ai_resp_json):\n         images = []\n         page_number = 1\n         for resp_json in ai_resp_json:\n-            document = resp_json.get(\"document\",{})\n+            document = resp_json.get(\"document\", {})\n             ai_pages = document.get(\"pages\", [])\n \n             for ai_page in ai_pages:"
        },
        {
            "sha": "2bd7f7649054e5a7878c8e2f9c7048d89ecff82d",
            "message": "Merge pull request #854 from Fellow-Consulting-AG/dev\n\nfd",
            "code_changes": "@@ -48,6 +48,18 @@ def populate_custom_lines(document):\n                     db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), 0.05\n                 )\n             )\n+\n+\n+            token_max_height = float(\n+                gdvh.get_float_value(\n+                    db, \"TOKEN_MAX_HEIGHT\", document.get(\"org_id\", None), 0.05\n+                )\n+            )\n+\n+            page[\"tokens\"] = [\n+                t for t in page[\"tokens\"] if abs(t.get(\"normalizedY1\", 0) - t.get(\"normalizedY0\", 0)) < token_max_height\n+            ]\n+\n             first_tokens = [\n                 t\n                 for t in page[\"tokens\"]"
        },
        {
            "sha": "7cf3be6478267bc146f38c2841368003789bd1c0",
            "message": "fd",
            "code_changes": "@@ -48,6 +48,18 @@ def populate_custom_lines(document):\n                     db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), 0.05\n                 )\n             )\n+\n+\n+            token_max_height = float(\n+                gdvh.get_float_value(\n+                    db, \"TOKEN_MAX_HEIGHT\", document.get(\"org_id\", None), 0.05\n+                )\n+            )\n+\n+            page[\"tokens\"] = [\n+                t for t in page[\"tokens\"] if abs(t.get(\"normalizedY1\", 0) - t.get(\"normalizedY0\", 0)) < token_max_height\n+            ]\n+\n             first_tokens = [\n                 t\n                 for t in page[\"tokens\"]"
        }
    ]
}