{
    "author": "Syed Amier Haider Shah",
    "week": "2023-W14",
    "total_commits": 2,
    "author_image": "https://avatars.githubusercontent.com/u/2101825?v=4",
    "commits": [
        {
            "sha": "3752ade3d44dfc3dcd1e961542878e1b689e42ad",
            "message": "fix",
            "code_changes": "@@ -111,6 +111,7 @@ class OCR_SERVICE_TYPES:\n     EASYOCR = \"EASYOCR\"\n     TESSERACT = \"TESSERACT\"\n     GOOGLE = \"GOOGLE\"\n+    AI = \"AI\"\n     PDF_MINER = \"PDF_MINER\"\n     E_TEXT = \"E_TEXT\"\n "
        },
        {
            "sha": "83520a21215fcab851de2767fdaf1a830102f15b",
            "message": "removed google",
            "code_changes": "@@ -1,76 +1,274 @@\n import json\n+import logging\n import tempfile\n import time\n from base64 import decodebytes\n \n import constants\n import helper.ai.ai_common as ai\n-import regex\n+from helper.util import find_related_locale\n from logger import get_logger\n+from models import OCRConfigurations\n+from util import sync_wrapper\n+\n+from .iocr import IOCRService\n \n logger = get_logger(\"ai_ocr\")\n \n url = \"https://eu-documentai.googleapis.com/v1/projects/342434080717/locations/eu/processors/4bfbf986c18da072:process\"\n \n \n-def check_for_existing_line(custom_lines, y0, y1):\n-    token_mid = (y1 + y0) / 2\n-    for line in custom_lines:\n-        if y0 < line[\"mid\"] and line[\"y0\"] < token_mid:\n-            return line\n-        elif line[\"y0\"] < token_mid and token_mid < line[\"y1\"]:\n-            return line\n-\n-    return None\n-\n-\n-def detect_language(page):\n-    if len(page.get(\"detectedLanguages\", [])) > 0:\n-        return page.get(\"detectedLanguages\")[0].get(\"languageCode\")\n-\n-\n-def generate_regex_from_string(string):\n-    string = (\n-        string.replace(\"\\\\\", \"\\\\\\\\\")\n-        .replace(\"+\", \"\\\\+\")\n-        .replace(\"*\", \"\\\\*\")\n-        .replace(\"^\", \"\\\\^\")\n-        .replace(\"|\", \"\\\\|\")\n-        .replace(\"?\", \"\\\\?\")\n-        .replace(\".\", \"\\\\.\")\n-        .replace(\"[\", \"\\\\[\")\n-        .replace(\"]\", \"\\\\]\")\n-        .replace(\"(\", \"\\\\(\")\n-        .replace(\")\", \"\\\\)\")\n-        .replace(\"{\", \"\\\\{\")\n-        .replace(\"}\", \"\\\\}\")\n-        .replace(\"-\", \"\\\\-\")\n-        .replace(\"$\", \"\\\\$\")\n-    )\n-    string = regex.sub(\"\\\\p{L}\", \"\\\\\\\\p{L}\", string)\n-    string = regex.sub(\"\\\\d{2,}(?![,.]?\\\\d*\\\\s?%)\", \"\\\\\\\\d+\", string)\n-    string = regex.sub(\"\\\\d(?!\\\\d?\\\\s?%)\", \"\\\\\\\\d\", string)\n-    string = regex.sub(\"\\\\s\", \"\\\\\\\\s?\", string)\n-    return string\n-\n-\n-def generate_hard_regex_from_string(string):\n-    string = (\n-        string.replace(\"\\\\\", \"\\\\\\\\\")\n-        .replace(\"+\", \"\\\\+\")\n-        .replace(\"*\", \"\\\\*\")\n-        .replace(\"^\", \"\\\\^\")\n-        .replace(\"|\", \"\\\\|\")\n-        .replace(\"?\", \"\\\\?\")\n-        .replace(\".\", \"\\\\.\")\n-        .replace(\"[\", \"\\\\[\")\n-        .replace(\"]\", \"\\\\]\")\n-        .replace(\"(\", \"\\\\(\")\n-        .replace(\")\", \"\\\\)\")\n-        .replace(\"{\", \"\\\\{\")\n-        .replace(\"}\", \"\\\\}\")\n-        .replace(\"-\", \"\\\\-\")\n-        .replace(\"$\", \"\\\\$\")\n-    )\n-    string = regex.sub(\"\\\\s\", \"\\\\\\\\s?\", string)\n-    return string\n+class AI_OCR_SERVICE(IOCRService):\n+    def __init__(self, config: OCRConfigurations, **kwargs):\n+        self.config: OCRConfigurations = config\n+        pass\n+\n+    def __get_ocr_data__(self, source_file_path, doc_id=None):\n+        ai_resp_json = ai.get_ai_response_v3(\n+            constants.AI_REQUEST_TYPE.OCR, url, source_file_path, None, doc_id\n+        )\n+        document = AI_OCR_SERVICE.transform_ai_response(ai_resp_json)\n+\n+        document[\"doc_lang\"] = \"de\"\n+        document[\"doc_origin\"] = \"DE\"\n+        document[\"doc_locale\"] = \"de_DE\"\n+        document[\"ocr_lang\"] = \"de\"\n+        document[\"ocr_processor\"] = \"ai-ocr-service\"\n+\n+        AI_OCR_SERVICE.populate_origin_and_locale(ai_resp_json, document)\n+\n+        return document, ai_resp_json\n+\n+    def get_ocr_data(self, user, source_file_path):\n+        document, _ = self.__get_ocr_data__(source_file_path, None)\n+        return document\n+\n+    @sync_wrapper(module=\"ocr\")\n+    def extract_ocr_data_and_images(\n+        self, storage_manager, source_file_path, destination_dir, doc_id, user\n+    ):\n+        document, ai_resp_json = self.__get_ocr_data__(source_file_path, None)\n+\n+        document[\"doc_id\"] = doc_id\n+        document[\"ocr_processor\"] = \"ai-ocr-service\"\n+\n+        logger.info(f\"Going to save extracted image and ocr document\")\n+        start_time = time.perf_counter()\n+        # extracting and saving images\n+        images_paths, quality = AI_OCR_SERVICE.extract_and_save_images(\n+            ai_resp_json, doc_id, storage_manager, destination_dir\n+        )\n+        document[\"images\"] = images_paths\n+        document[\"qualitycheck\"] = quality\n+        end_time = time.perf_counter()\n+        logger.customer(\"Document successfully OCRed with AI\")\n+        logger.customer(f\"Saved document and images.\")\n+        return document\n+\n+    def transform_ai_response(ai_resp_json):\n+        # document = {\"text\": \"\"}\n+        document = {}\n+        if not ai_resp_json:\n+            return document\n+\n+        pages = []\n+\n+        indexIncrement = 0\n+        pageNumberIncrement = 0\n+        for resp_json in ai_resp_json:\n+            hocr_content = resp_json.get(\"document\",{}).get(\"text\", \"\")\n+            pageStartIndex = 0\n+            pageEndIndex = 0\n+            for index, page in enumerate(resp_json.get(\"document\",{}).get(\"pages\", [])):\n+                if (\n+                    not \"width\" in page.get(\"dimension\", {}) and not \"width\" in page.get(\"image\", {})\n+                ) or (\n+                    not \"height\" in page.get(\"dimension\", {}) and not \"height\" in page.get(\"image\", {})\n+                ):\n+                    continue\n+                width = page[\"dimension\"].get(\"width\", page[\"image\"][\"width\"])\n+                height = page[\"dimension\"].get(\"height\", page[\"image\"][\"height\"])\n+\n+                first_text_segment = page[\"layout\"][\"textAnchor\"][\"textSegments\"][0]\n+\n+                if \"startIndex\" in first_text_segment:\n+                    pageStartIndex = int(first_text_segment[\"startIndex\"])\n+                elif index > 0:\n+                    pass\n+                    # previous_page = resp_json.get(\"document\",{})[\"pages\"][index-1]\n+                    # previous_page_first_text_segment = previous_page[\"layout\"][\"textAnchor\"][\"textSegments\"][0]\n+                    # pageStartIndex = int(pages[len(pages)-1]['endIndex'])\n+\n+                if \"endIndex\" in first_text_segment:\n+                    pageEndIndex = int(first_text_segment[\"endIndex\"])\n+\n+                current_page = {\n+                    \"startIndex\": pageStartIndex + indexIncrement,\n+                    \"endIndex\": pageEndIndex + indexIncrement,\n+                    \"pageText\": hocr_content[pageStartIndex:pageEndIndex],\n+                    \"pageNumber\": page[\"pageNumber\"] + pageNumberIncrement,\n+                    \"width\": width,\n+                    \"height\": height,\n+                    # \"indexIncrement\": indexIncrement,\n+                }\n+                # sections = [\"blocks\", \"paragraphs\", \"lines\", \"tokens\"]\n+                sections = [\"tokens\"]\n+                for section in sections:\n+                    adjustment_factor = 0\n+                    items = []\n+                    if section not in page:\n+                        current_page[section] = items\n+                        continue\n+                    for item in page[section]:\n+                        textSegment = item[\"layout\"][\"textAnchor\"][\"textSegments\"][0]\n+                        startIndex = 0\n+                        if \"startIndex\" in textSegment:\n+                            startIndex = int(textSegment[\"startIndex\"])\n+\n+                        endIndex = 0\n+                        if \"endIndex\" in textSegment:\n+                            endIndex = int(textSegment[\"endIndex\"])\n+\n+                        startIndex = startIndex + adjustment_factor\n+                        endIndex = endIndex + adjustment_factor\n+\n+                        if (\n+                            section == \"tokens\"\n+                            and \"\\n\" in hocr_content[startIndex:endIndex].rstrip()\n+                        ):\n+                            adjustment_factor = adjustment_factor + 1\n+                            startIndex += 1\n+                            endIndex += 1\n+\n+                        text = hocr_content[startIndex:endIndex].strip()\n+                        nv0 = item[\"layout\"][\"boundingPoly\"][\"normalizedVertices\"][0]\n+                        nv2 = item[\"layout\"][\"boundingPoly\"][\"normalizedVertices\"][2]\n+                        if (\n+                            \"x\" not in nv0\n+                            or \"x\" not in nv2\n+                            or \"y\" not in nv0\n+                            or \"y\" not in nv2\n+                            or nv2[\"x\"] < 0\n+                            or nv0[\"x\"] < 0\n+                            or nv2[\"y\"] < 0\n+                            or nv0[\"y\"] < 0\n+                            or nv2[\"x\"] > 1\n+                            or nv0[\"x\"] > 1\n+                            or nv2[\"y\"] > 1\n+                            or nv0[\"y\"] > 1\n+                            or nv2[\"x\"] < nv0[\"x\"]\n+                            or nv2[\"y\"]\n+                            < nv0[\"y\"]  # to remove garbage for table extraction\n+                        ):\n+                            continue\n+\n+                        confidence = round(item[\"layout\"].get(\"confidence\", 1) * 100)\n+                        items.append(\n+                            {\n+                                \"text\": text,\n+                                \"startIndex\": startIndex + indexIncrement,\n+                                \"endIndex\": endIndex + indexIncrement,\n+                                \"confidence\": confidence,\n+                                \"x0\": round(nv0[\"x\"] * width),\n+                                \"y0\": round(nv0[\"y\"] * height),\n+                                \"x1\": round(nv2[\"x\"] * width),\n+                                \"y1\": round(nv2[\"y\"] * height),\n+                                \"normalizedX0\": nv0[\"x\"],\n+                                \"normalizedY0\": nv0[\"y\"],\n+                                \"normalizedX1\": nv2[\"x\"],\n+                                \"normalizedY1\": nv2[\"y\"],\n+                                # \"normalizedVertices\": item[\"layout\"][\"boundingPoly\"][\n+                                #     \"normalizedVertices\"\n+                                # ],\n+                            }\n+                        )\n+\n+                    current_page[section] = items\n+\n+                pages.append(current_page)\n+\n+                if page[\"pageNumber\"] == 10:\n+                    indexIncrement = indexIncrement + pageEndIndex\n+                    pageNumberIncrement = pageNumberIncrement + 10\n+                    pageStartIndex = 0\n+                    pageEndIndex = 0\n+\n+            # document[\"text\"] += hocr_content\n+        document[\"pages\"] = pages\n+\n+        return document\n+\n+    def get_detailed_images(ai_resp_json):\n+        images = []\n+        page_number = 1\n+        for resp_json in ai_resp_json:\n+            document = resp_json.get(\"document\",{})\n+            ai_pages = document.get(\"pages\", [])\n+\n+            for ai_page in ai_pages:\n+                image = ai_page[\"image\"]\n+                image[\"pageNumber\"] = page_number\n+                images.append(image)\n+                page_number += 1\n+\n+        return images\n+\n+    def extract_and_save_images(ai_resp_json, doc_id, storage_manager, dest_dir):\n+        images_paths = []\n+        doc_quality = []\n+        counter = 0\n+        ai_images = AI_OCR_SERVICE.get_detailed_images(ai_resp_json)\n+\n+        image_dest_path = dest_dir + \"/images\"\n+\n+        for ai_image in ai_images:\n+            page = str(ai_image[\"pageNumber\"])\n+            img_data = decodebytes(ai_image[\"content\"].encode())\n+            storage_manager.save_file_data(img_data, f\"{page}.png\", image_dest_path)\n+            img_data = {\n+                \"path\": f\"docbits/images/{doc_id}/{page}.png\",\n+                \"filename\": f\"{page}.png\",\n+                \"pageNumber\": ai_image[\"pageNumber\"],\n+                \"height\": ai_image[\"height\"],\n+                \"width\": ai_image[\"width\"],\n+            }\n+\n+            if \"entities\" in ai_resp_json[0][\"document\"]:\n+                for entity in ai_resp_json[0][\"document\"][\"entities\"]:\n+                    conf_percent = entity[\"confidence\"]\n+                    doc_quality.append(\n+                        {\"page\": page, \"overall\": f\"{conf_percent*100}%\"}\n+                    )\n+\n+                    for prop in entity[\"properties\"]:\n+                        if \"confidence\" in prop:\n+                            conf_percent = prop[\"confidence\"]\n+                            doc_quality[counter][prop[\"type\"]] = f\"{conf_percent*100}%\"\n+                    break\n+                counter += 1\n+\n+            images_paths.append(img_data)\n+        return images_paths, doc_quality\n+\n+    def save_ocr_content(document, doc_id, storage_manager, dest_dir):\n+        with tempfile.NamedTemporaryFile(\n+            suffix=\".json\", mode=\"w\", encoding=\"utf8\"\n+        ) as outfile:\n+            outfile.write(json.dumps(document) + \"\\n\")\n+            outfile.seek(0)\n+            storage_manager.save_file(\n+                outfile.name, doc_id + \"_ai_transform.json\", dest_dir\n+            )\n+\n+    def populate_origin_and_locale(ai_resp, document):\n+        ai_resp_document = ai_resp[0][\"document\"]\n+        first_page = ai_resp_document[\"pages\"][0]\n+\n+        document[\"doc_lang\"] = \"de\"\n+        if len(first_page.get(\"detectedLanguages\", [])) > 0:\n+            document[\"doc_lang\"] = first_page.get(\"detectedLanguages\")[0].get(\n+                \"languageCode\"\n+            )\n+        document[\"ocr_lang\"] = document[\"doc_lang\"]\n+        doc_locale = document.get(\"doc_lang\") + \"_\" + document.get(\"doc_origin\")\n+        document[\"doc_locale\"] = find_related_locale(doc_locale, prefer_lang=True)"
        }
    ]
}