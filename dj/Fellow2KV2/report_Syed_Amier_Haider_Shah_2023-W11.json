{
    "author": "Syed Amier Haider Shah",
    "week": "2023-W11",
    "total_commits": 30,
    "author_image": "https://avatars.githubusercontent.com/u/2101825?v=4",
    "commits": [
        {
            "sha": "0c6d32861f41d0695980cfd388649547b77c97e0",
            "message": "Merge pull request #835 from Fellow-Consulting-AG/stage\n\nStage",
            "code_changes": "@@ -4,8 +4,10 @@\n \n from shapely.geometry import Polygon\n \n-from helper.form_extractor.fill_ratio import (get_field_fill_ratio,\n-                                              get_opencv_image_from_url)\n+from helper.form_extractor.fill_ratio import (\n+    get_field_fill_ratio,\n+    get_opencv_image_from_url,\n+)\n from helper.util import clean_amount_fields, clean_string\n \n sys.path.append(\".\")\n@@ -15,9 +17,9 @@\n \n import fellow2kv.config as config\n from fellow2kv.extension import storagemanager_doc2\n-from helper.common_regex import (CURRENCY_REGEX,\n-                                 NUMERIC_AND_AMOUNT_SEPARATORS_REGEX)\n+from helper.common_regex import CURRENCY_REGEX, NUMERIC_AND_AMOUNT_SEPARATORS_REGEX\n from helper.extract_util_errors import InvalidDirectionError, UndifiendError\n+\n # from fellow2kv.extension storagemanager_doc2\n from logger import get_logger\n \n@@ -1327,9 +1329,8 @@ def is_multi_line(tokens):\n     return True\n \n \n-\n def remove_duplicate_tokens(tokens):\n-    entries_to_remove = ('startIndex', 'endIndex', 'customStartIndex', 'customEndIndex')\n+    entries_to_remove = (\"startIndex\", \"endIndex\", \"customStartIndex\", \"customEndIndex\")\n     filtered_tokens = []\n     seen = set()\n     for token in tokens:"
        },
        {
            "sha": "754158f2da3671ab6efc544c3be5f4bf0266c417",
            "message": "Merge pull request #834 from Fellow-Consulting-AG/dev\n\nfix",
            "code_changes": "@@ -1678,10 +1678,9 @@ def get(self):\n             field_names = [fr[\"field_name\"] for fr in field_rules]\n             field_names = \", \".join([f\"'{value}'\" for value in field_names])\n \n-            query2 = (\n-                base_query\n-                + f\" and fvr.org_id = 'DEFAULT' and fvr.field_name not in ({field_names})\"\n-            )\n+            query2 = base_query + f\" and fvr.org_id = 'DEFAULT'\"\n+            if field_names:\n+                query2 = query2 + f\" and fvr.field_name not in ({field_names})\"\n             default_field_rules = dbh.execute_query(db, query2)\n \n             field_rules = field_rules + default_field_rules"
        },
        {
            "sha": "a151e962abe12d50b80311dc12c51ac520f3eaf5",
            "message": "Merge branch 'stage' into dev",
            "code_changes": "@@ -4,8 +4,10 @@\n \n from shapely.geometry import Polygon\n \n-from helper.form_extractor.fill_ratio import (get_field_fill_ratio,\n-                                              get_opencv_image_from_url)\n+from helper.form_extractor.fill_ratio import (\n+    get_field_fill_ratio,\n+    get_opencv_image_from_url,\n+)\n from helper.util import clean_amount_fields, clean_string\n \n sys.path.append(\".\")\n@@ -15,9 +17,9 @@\n \n import fellow2kv.config as config\n from fellow2kv.extension import storagemanager_doc2\n-from helper.common_regex import (CURRENCY_REGEX,\n-                                 NUMERIC_AND_AMOUNT_SEPARATORS_REGEX)\n+from helper.common_regex import CURRENCY_REGEX, NUMERIC_AND_AMOUNT_SEPARATORS_REGEX\n from helper.extract_util_errors import InvalidDirectionError, UndifiendError\n+\n # from fellow2kv.extension storagemanager_doc2\n from logger import get_logger\n \n@@ -1327,9 +1329,8 @@ def is_multi_line(tokens):\n     return True\n \n \n-\n def remove_duplicate_tokens(tokens):\n-    entries_to_remove = ('startIndex', 'endIndex', 'customStartIndex', 'customEndIndex')\n+    entries_to_remove = (\"startIndex\", \"endIndex\", \"customStartIndex\", \"customEndIndex\")\n     filtered_tokens = []\n     seen = set()\n     for token in tokens:"
        },
        {
            "sha": "616b588f68203b231636ce5513737d82960cdf6d",
            "message": "fix",
            "code_changes": "@@ -1670,7 +1670,9 @@ def get(self):\n             field_names = [fr[\"field_name\"] for fr in field_rules]\n             field_names = ', '.join([f\"'{value}'\" for value in field_names])\n \n-            query2 = base_query + f\" and fvr.org_id = 'DEFAULT' and fvr.field_name not in ({field_names})\"\n+            query2 = base_query + f\" and fvr.org_id = 'DEFAULT'\"\n+            if field_names:\n+                query2 = query2 + f\" and fvr.field_name not in ({field_names})\"\n             default_field_rules = dbh.execute_query(db, query2)\n \n             field_rules = field_rules + default_field_rules"
        },
        {
            "sha": "1076cec8c964947d95a19bc96fed17cde18ab434",
            "message": "Merge pull request #833 from Fellow-Consulting-AG/stage\n\nduplicate token fix",
            "code_changes": "@@ -4,23 +4,20 @@\n \n from shapely.geometry import Polygon\n \n-from helper.form_extractor.fill_ratio import (\n-    get_field_fill_ratio,\n-    get_opencv_image_from_url,\n-)\n+from helper.form_extractor.fill_ratio import (get_field_fill_ratio,\n+                                              get_opencv_image_from_url)\n from helper.util import clean_amount_fields, clean_string\n \n sys.path.append(\".\")\n import math\n-import time\n \n import regex as re\n \n import fellow2kv.config as config\n from fellow2kv.extension import storagemanager_doc2\n-from helper.common_regex import CURRENCY_REGEX, NUMERIC_AND_AMOUNT_SEPARATORS_REGEX\n+from helper.common_regex import (CURRENCY_REGEX,\n+                                 NUMERIC_AND_AMOUNT_SEPARATORS_REGEX)\n from helper.extract_util_errors import InvalidDirectionError, UndifiendError\n-\n # from fellow2kv.extension storagemanager_doc2\n from logger import get_logger\n \n@@ -623,6 +620,10 @@ def get_string_from_coordinates(\n         tokens = self.get_tokens_in_coordinates_range(\n             coordinates, page_num, use_custom_tokens\n         )\n+\n+        # added remove duplicate tokens as a fix for the issue where the same token is returned twice\n+        tokens = remove_duplicate_tokens(tokens)\n+\n         return self.get_match_data_from_matched_tokens(\n             page_num, tokens, append_tokens=append_tokens\n         )\n@@ -1324,3 +1325,20 @@ def is_multi_line(tokens):\n         return False\n \n     return True\n+\n+\n+\n+def remove_duplicate_tokens(tokens):\n+    entries_to_remove = ('startIndex', 'endIndex', 'customStartIndex', 'customEndIndex')\n+    filtered_tokens = []\n+    seen = set()\n+    for token in tokens:\n+        token_copy = copy.deepcopy(token)\n+        for k in entries_to_remove:\n+            token_copy.pop(k, None)\n+\n+        t = tuple(token_copy.items())\n+        if t not in seen:\n+            seen.add(t)\n+            filtered_tokens.append(token)\n+    return filtered_tokens"
        },
        {
            "sha": "e9487baaf2505773a48c5406789112802a8b0f00",
            "message": "Merge pull request #832 from Fellow-Consulting-AG/dev\n\nfix",
            "code_changes": "@@ -4,23 +4,20 @@\n \n from shapely.geometry import Polygon\n \n-from helper.form_extractor.fill_ratio import (\n-    get_field_fill_ratio,\n-    get_opencv_image_from_url,\n-)\n+from helper.form_extractor.fill_ratio import (get_field_fill_ratio,\n+                                              get_opencv_image_from_url)\n from helper.util import clean_amount_fields, clean_string\n \n sys.path.append(\".\")\n import math\n-import time\n \n import regex as re\n \n import fellow2kv.config as config\n from fellow2kv.extension import storagemanager_doc2\n-from helper.common_regex import CURRENCY_REGEX, NUMERIC_AND_AMOUNT_SEPARATORS_REGEX\n+from helper.common_regex import (CURRENCY_REGEX,\n+                                 NUMERIC_AND_AMOUNT_SEPARATORS_REGEX)\n from helper.extract_util_errors import InvalidDirectionError, UndifiendError\n-\n # from fellow2kv.extension storagemanager_doc2\n from logger import get_logger\n \n@@ -623,6 +620,10 @@ def get_string_from_coordinates(\n         tokens = self.get_tokens_in_coordinates_range(\n             coordinates, page_num, use_custom_tokens\n         )\n+\n+        # added remove duplicate tokens as a fix for the issue where the same token is returned twice\n+        tokens = remove_duplicate_tokens(tokens)\n+\n         return self.get_match_data_from_matched_tokens(\n             page_num, tokens, append_tokens=append_tokens\n         )\n@@ -1324,3 +1325,20 @@ def is_multi_line(tokens):\n         return False\n \n     return True\n+\n+\n+\n+def remove_duplicate_tokens(tokens):\n+    entries_to_remove = ('startIndex', 'endIndex', 'customStartIndex', 'customEndIndex')\n+    filtered_tokens = []\n+    seen = set()\n+    for token in tokens:\n+        token_copy = copy.deepcopy(token)\n+        for k in entries_to_remove:\n+            token_copy.pop(k, None)\n+\n+        t = tuple(token_copy.items())\n+        if t not in seen:\n+            seen.add(t)\n+            filtered_tokens.append(token)\n+    return filtered_tokens"
        },
        {
            "sha": "745632eb1d17db006b60ef7156ac4c5260f11948",
            "message": "remove duplicate tokens",
            "code_changes": "@@ -4,23 +4,20 @@\n \n from shapely.geometry import Polygon\n \n-from helper.form_extractor.fill_ratio import (\n-    get_field_fill_ratio,\n-    get_opencv_image_from_url,\n-)\n+from helper.form_extractor.fill_ratio import (get_field_fill_ratio,\n+                                              get_opencv_image_from_url)\n from helper.util import clean_amount_fields, clean_string\n \n sys.path.append(\".\")\n import math\n-import time\n \n import regex as re\n \n import fellow2kv.config as config\n from fellow2kv.extension import storagemanager_doc2\n-from helper.common_regex import CURRENCY_REGEX, NUMERIC_AND_AMOUNT_SEPARATORS_REGEX\n+from helper.common_regex import (CURRENCY_REGEX,\n+                                 NUMERIC_AND_AMOUNT_SEPARATORS_REGEX)\n from helper.extract_util_errors import InvalidDirectionError, UndifiendError\n-\n # from fellow2kv.extension storagemanager_doc2\n from logger import get_logger\n \n@@ -623,6 +620,10 @@ def get_string_from_coordinates(\n         tokens = self.get_tokens_in_coordinates_range(\n             coordinates, page_num, use_custom_tokens\n         )\n+\n+        # added remove duplicate tokens as a fix for the issue where the same token is returned twice\n+        tokens = remove_duplicate_tokens(tokens)\n+\n         return self.get_match_data_from_matched_tokens(\n             page_num, tokens, append_tokens=append_tokens\n         )\n@@ -1324,3 +1325,20 @@ def is_multi_line(tokens):\n         return False\n \n     return True\n+\n+\n+\n+def remove_duplicate_tokens(tokens):\n+    entries_to_remove = ('startIndex', 'endIndex', 'customStartIndex', 'customEndIndex')\n+    filtered_tokens = []\n+    seen = set()\n+    for token in tokens:\n+        token_copy = copy.deepcopy(token)\n+        for k in entries_to_remove:\n+            token_copy.pop(k, None)\n+\n+        t = tuple(token_copy.items())\n+        if t not in seen:\n+            seen.add(t)\n+            filtered_tokens.append(token)\n+    return filtered_tokens"
        },
        {
            "sha": "f1a80ca0c3e0ee783f376770730dc73550655cac",
            "message": "Merge pull request #831 from Fellow-Consulting-AG/stage\n\nfix1",
            "code_changes": "@@ -180,9 +180,9 @@ def __classify_document__(\n         )\n         doc_type_csn, doc_locale_csn = tfidf_classifier.classify_document(document)\n \n-        if not doc_type_csn.value in active_doc_types:\n+        if doc_type_csn and not doc_type_csn.value in active_doc_types:\n             doc_type_csn = None\n-        \n+\n         if doc_type_csn:\n             resp_json[\"flow_meta\"].append(\n                 {"
        },
        {
            "sha": "d66a71a5ec38b891bf87f388edeabc651123994d",
            "message": "Merge pull request #830 from Fellow-Consulting-AG/dev\n\nfix 1",
            "code_changes": "@@ -180,7 +180,7 @@ def __classify_document__(\n         )\n         doc_type_csn, doc_locale_csn = tfidf_classifier.classify_document(document)\n \n-        if not doc_type_csn.value in active_doc_types:\n+        if doc_type_csn and not doc_type_csn.value in active_doc_types:\n             doc_type_csn = None\n \n         if doc_type_csn:"
        },
        {
            "sha": "68e635a3e28138d6af60e00d8f044b54145143c5",
            "message": "fix 1",
            "code_changes": "@@ -180,7 +180,7 @@ def __classify_document__(\n         )\n         doc_type_csn, doc_locale_csn = tfidf_classifier.classify_document(document)\n \n-        if not doc_type_csn.value in active_doc_types:\n+        if doc_type_csn and not doc_type_csn.value in active_doc_types:\n             doc_type_csn = None\n \n         if doc_type_csn:"
        },
        {
            "sha": "6e65df0a828568de9f217be77ed5b2cb31c47483",
            "message": "Merge pull request #829 from Fellow-Consulting-AG/stage\n\nStage",
            "code_changes": "@@ -203,7 +203,10 @@ def get_highest_priority_label(model_labels, result):\n         # first label returned, if there is no entry for the classified label found\n         result_page = result_pages_all[0]\n         cr = ClassificationResult(\n-            result_page[\"classification\"], result_page[\"page\"], result_page[\"prob\"], f\"First label\"\n+            result_page[\"classification\"],\n+            result_page[\"page\"],\n+            result_page[\"prob\"],\n+            f\"First label\",\n         )\n         return cr\n "
        },
        {
            "sha": "300b35a95077758f3e047f5e74252d184e7b173c",
            "message": "Merge pull request #828 from Fellow-Consulting-AG/dev\n\nfix",
            "code_changes": "@@ -153,6 +153,7 @@ def __classify_document__(\n         doc_type_csn = keyword_classifier.detect_document_type(\n             ai_extractor, user, keyword_types\n         )\n+\n         if doc_type_csn:\n             resp_json[\"flow_meta\"].append(\n                 {\n@@ -179,20 +180,23 @@ def __classify_document__(\n         )\n         doc_type_csn, doc_locale_csn = tfidf_classifier.classify_document(document)\n \n-        resp_json[\"flow_meta\"].append(\n-            {\n-                \"name\": \"classification\",\n-                \"type\": \"classify\",\n-                \"data\": \"true\",\n-                \"result\": {\n-                    \"doc_type_csn\": doc_type_csn,\n-                    \"doc_locale_csn\": doc_locale_csn,\n-                },\n-                \"module\": \"classify\",\n-                \"message\": \"TF-IDF-Classification successful\",\n-            }\n-        )\n+        if not doc_type_csn.value in active_doc_types:\n+            doc_type_csn = None\n+        \n         if doc_type_csn:\n+            resp_json[\"flow_meta\"].append(\n+                {\n+                    \"name\": \"classification\",\n+                    \"type\": \"classify\",\n+                    \"data\": \"true\",\n+                    \"result\": {\n+                        \"doc_type_csn\": doc_type_csn,\n+                        \"doc_locale_csn\": doc_locale_csn,\n+                    },\n+                    \"module\": \"classify\",\n+                    \"message\": \"TF-IDF-Classification successful\",\n+                }\n+            )\n             result_value = doc_type_csn.value\n             result_score = doc_type_csn.score * 100\n             logger.customer("
        },
        {
            "sha": "a4e84f00afa7579b38f42a372f5bfc1248d38dd1",
            "message": "Merge branch 'stage' into dev",
            "code_changes": "@@ -1 +1 @@\n-2.0.497\n\\ No newline at end of file\n+2.0.497"
        },
        {
            "sha": "8fd98345a30cdcc7d388c2ddabbe500b200f149b",
            "message": "fix",
            "code_changes": "@@ -153,6 +153,7 @@ def __classify_document__(\n         doc_type_csn = keyword_classifier.detect_document_type(\n             ai_extractor, user, keyword_types\n         )\n+\n         if doc_type_csn:\n             resp_json[\"flow_meta\"].append(\n                 {\n@@ -179,20 +180,23 @@ def __classify_document__(\n         )\n         doc_type_csn, doc_locale_csn = tfidf_classifier.classify_document(document)\n \n-        resp_json[\"flow_meta\"].append(\n-            {\n-                \"name\": \"classification\",\n-                \"type\": \"classify\",\n-                \"data\": \"true\",\n-                \"result\": {\n-                    \"doc_type_csn\": doc_type_csn,\n-                    \"doc_locale_csn\": doc_locale_csn,\n-                },\n-                \"module\": \"classify\",\n-                \"message\": \"TF-IDF-Classification successful\",\n-            }\n-        )\n+        if not doc_type_csn.value in active_doc_types:\n+            doc_type_csn = None\n+        \n         if doc_type_csn:\n+            resp_json[\"flow_meta\"].append(\n+                {\n+                    \"name\": \"classification\",\n+                    \"type\": \"classify\",\n+                    \"data\": \"true\",\n+                    \"result\": {\n+                        \"doc_type_csn\": doc_type_csn,\n+                        \"doc_locale_csn\": doc_locale_csn,\n+                    },\n+                    \"module\": \"classify\",\n+                    \"message\": \"TF-IDF-Classification successful\",\n+                }\n+            )\n             result_value = doc_type_csn.value\n             result_score = doc_type_csn.score * 100\n             logger.customer("
        },
        {
            "sha": "133552b6ff2121c68cec4b7dc289311db0f90787",
            "message": "Merge pull request #827 from Fellow-Consulting-AG/stage\n\nStage",
            "code_changes": "@@ -42,13 +42,12 @@ def populate_custom_lines(document):\n             db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), True\n         )\n \n-        remove_boundry_width_percentage = float(\n-            gdvh.get_float_value(\n-                db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), 0.05\n-            )\n-        )\n-\n         if remove_boundry_tokens:\n+            remove_boundry_width_percentage = float(\n+                gdvh.get_float_value(\n+                    db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), 0.05\n+                )\n+            )\n             first_tokens = [\n                 t\n                 for t in page[\"tokens\"]"
        },
        {
            "sha": "98ab95ce9794e148dab23c7cbea87018e42a3daa",
            "message": "Merge branch 'sandbox' into stage",
            "code_changes": "@@ -1 +1 @@\n-2.0.495\n\\ No newline at end of file\n+2.0.495"
        },
        {
            "sha": "3f7ccf42784f98ee3939b4fd77fc50bd86737154",
            "message": "Merge pull request #826 from Fellow-Consulting-AG/dev\n\nfix",
            "code_changes": "@@ -42,13 +42,12 @@ def populate_custom_lines(document):\n             db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), True\n         )\n \n-        remove_boundry_width_percentage = float(\n-            gdvh.get_float_value(\n-                db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), 0.05\n-            )\n-        )\n-\n         if remove_boundry_tokens:\n+            remove_boundry_width_percentage = float(\n+                gdvh.get_float_value(\n+                    db, \"REMOVE_BOUNDRY_TOKENS\", document.get(\"org_id\", None), 0.05\n+                )\n+            )\n             first_tokens = [\n                 t\n                 for t in page[\"tokens\"]"
        },
        {
            "sha": "f8edf496a10f8b5d970aeb28282525c9b3069ddd",
            "message": "classification fix",
            "code_changes": "@@ -153,22 +153,23 @@ def __classify_document__(\n         doc_type_csn = keyword_classifier.detect_document_type(\n             ai_extractor, user, keyword_types\n         )\n-        resp_json[\"flow_meta\"].append(\n-            {\n-                \"name\": \"classification\",\n-                \"type\": \"classify\",\n-                \"data\": \"true\",\n-                \"result\": {\"doc_type_csn\": doc_type_csn},\n-                \"module\": \"classify\",\n-                \"message\": \"Keyword-Classification successful\",\n-            }\n-        )\n-        result_value = doc_type_csn.value\n-        result_score = doc_type_csn.score * 100\n+        if doc_type_csn:\n+            resp_json[\"flow_meta\"].append(\n+                {\n+                    \"name\": \"classification\",\n+                    \"type\": \"classify\",\n+                    \"data\": \"true\",\n+                    \"result\": {\"doc_type_csn\": doc_type_csn},\n+                    \"module\": \"classify\",\n+                    \"message\": \"Keyword-Classification successful\",\n+                }\n+            )\n+            result_value = doc_type_csn.value\n+            result_score = doc_type_csn.score * 100\n \n-        logger.customer(\n-            f\"Keyword-Classification successful. Classified as {result_value} with {result_score:.2f}% confidence.\"\n-        )\n+            logger.customer(\n+                f\"Keyword-Classification successful. Classified as {result_value} with {result_score:.2f}% confidence.\"\n+            )\n \n     # Classifying based on TF-IDF and Trained Model\n     if not doc_type_csn and not model_only:"
        },
        {
            "sha": "4a573a8e601bff5ab451076d54c79dd52cca4138",
            "message": "Merge pull request #825 from Fellow-Consulting-AG/dev\n\nclassification fix",
            "code_changes": "@@ -76,7 +76,7 @@ def classify_document(user: UserAuthentication, model, local_file_path, pages=No\n \n     # Getting classified label with highest priority. If there are multiple high priority labels, we get default label\n     res: ClassificationResult = get_highest_priority_label(model_labels, result)\n-    if res.score == -1:\n+    if res.score == -1 or not res.label in model_labels:\n         result[\"classification_label\"] = res.label\n         result[\"classification_message\"] = res.message\n         result[\"classification_score\"] = res.score\n@@ -203,7 +203,7 @@ def get_highest_priority_label(model_labels, result):\n         # first label returned, if there is no entry for the classified label found\n         result_page = result_pages_all[0]\n         cr = ClassificationResult(\n-            result_page[\"classification\"], result_page[\"page\"], -1, f\"First label\"\n+            result_page[\"classification\"], result_page[\"page\"], result_page[\"prob\"], f\"First label\"\n         )\n         return cr\n "
        },
        {
            "sha": "c019d859e8214ee5220171145907e51b421a5bd9",
            "message": "Merge branch 'stage' into dev",
            "code_changes": "@@ -4,8 +4,9 @@\n from authenticator import UserAuthentication\n from constants import CLASSIFICATION_MODELS\n from fellow2kv.extension import db\n-from helper.custom_classifier_v2.iclassifier_model_processor import \\\n-    IClassifierModelProcessor\n+from helper.custom_classifier_v2.iclassifier_model_processor import (\n+    IClassifierModelProcessor,\n+)\n from util import sync_wrapper\n \n from . import common, mlp_classifier_processor, service_classifier_processor"
        },
        {
            "sha": "8db8f50ffaf4822e9336de1a0502af6e93bbf124",
            "message": "classified fix",
            "code_changes": "@@ -4,9 +4,8 @@\n from authenticator import UserAuthentication\n from constants import CLASSIFICATION_MODELS\n from fellow2kv.extension import db\n-from helper.custom_classifier_v2.iclassifier_model_processor import (\n-    IClassifierModelProcessor,\n-)\n+from helper.custom_classifier_v2.iclassifier_model_processor import \\\n+    IClassifierModelProcessor\n from util import sync_wrapper\n \n from . import common, mlp_classifier_processor, service_classifier_processor\n@@ -76,7 +75,7 @@ def classify_document(user: UserAuthentication, model, local_file_path, pages=No\n \n     # Getting classified label with highest priority. If there are multiple high priority labels, we get default label\n     res: ClassificationResult = get_highest_priority_label(model_labels, result)\n-    if res.score == -1:\n+    if res.score == -1 or not res.label in model_labels:\n         result[\"classification_label\"] = res.label\n         result[\"classification_message\"] = res.message\n         result[\"classification_score\"] = res.score\n@@ -203,7 +202,7 @@ def get_highest_priority_label(model_labels, result):\n         # first label returned, if there is no entry for the classified label found\n         result_page = result_pages_all[0]\n         cr = ClassificationResult(\n-            result_page[\"classification\"], result_page[\"page\"], -1, f\"First label\"\n+            result_page[\"classification\"], result_page[\"page\"], result_page[\"prob\"], f\"First label\"\n         )\n         return cr\n "
        },
        {
            "sha": "16db427173b0aa87518f9fb6d4fe524a3db1ad4b",
            "message": "Merge pull request #820 from Fellow-Consulting-AG/stage\n\n0 prio flach label",
            "code_changes": "@@ -4,8 +4,9 @@\n from authenticator import UserAuthentication\n from constants import CLASSIFICATION_MODELS\n from fellow2kv.extension import db\n-from helper.custom_classifier_v2.iclassifier_model_processor import \\\n-    IClassifierModelProcessor\n+from helper.custom_classifier_v2.iclassifier_model_processor import (\n+    IClassifierModelProcessor,\n+)\n from util import sync_wrapper\n \n from . import common, mlp_classifier_processor, service_classifier_processor\n@@ -188,7 +189,7 @@ def get_highest_priority_label(model_labels, result):\n             page[\"label_doc_type\"] = doc_type\n             page[\"label_priority\"] = priority\n \n-    # getting classification labels with priority > 0\n+    # getting classification labels with priority >= 0\n     result_pages_all = copy.deepcopy(result[\"pages\"])\n     result_pages_all = [cl for cl in result_pages_all if page[\"classification\"]]\n     classified_pages = [\n@@ -199,13 +200,22 @@ def get_highest_priority_label(model_labels, result):\n         return None\n \n     if not classified_pages:\n-        # first label returned\n+        # first label returned, if there is no entry for the classified label found\n         result_page = result_pages_all[0]\n         cr = ClassificationResult(\n-            result_page[\"classification\"], result_page, -1, f\"First label\"\n+            result_page[\"classification\"], result_page[\"page\"], -1, f\"First label\"\n         )\n         return cr\n \n+    # If there is any label with priority 0. It should be overall label for the document\n+    # irrespective of other labels and their scores.\n+    for cp in classified_pages:\n+        if cp.get(\"label_priority\", -1) == 0:\n+            cr = ClassificationResult(\n+                cp[\"classification\"], cp[\"page\"], cp[\"prob\"], f\"0 Priority Label\"\n+            )\n+            return cr\n+\n     classified_pages = sorted(classified_pages, key=lambda x: x[\"label_priority\"])\n \n     high_priority_labels = ["
        },
        {
            "sha": "a0759f9b68779164199c8fed3732223a11af9f54",
            "message": "Merge pull request #819 from Fellow-Consulting-AG/dev\n\n0 prio flach label",
            "code_changes": "@@ -189,7 +189,7 @@ def get_highest_priority_label(model_labels, result):\n             page[\"label_doc_type\"] = doc_type\n             page[\"label_priority\"] = priority\n \n-    # getting classification labels with priority > 0\n+    # getting classification labels with priority >= 0\n     result_pages_all = copy.deepcopy(result[\"pages\"])\n     result_pages_all = [cl for cl in result_pages_all if page[\"classification\"]]\n     classified_pages = [\n@@ -200,13 +200,22 @@ def get_highest_priority_label(model_labels, result):\n         return None\n \n     if not classified_pages:\n-        # first label returned\n+        # first label returned, if there is no entry for the classified label found\n         result_page = result_pages_all[0]\n         cr = ClassificationResult(\n-            result_page[\"classification\"], result_page, -1, f\"First label\"\n+            result_page[\"classification\"], result_page[\"page\"], -1, f\"First label\"\n         )\n         return cr\n \n+    # If there is any label with priority 0. It should be overall label for the document\n+    # irrespective of other labels and their scores.\n+    for cp in classified_pages:\n+        if cp.get(\"label_priority\", -1) == 0:\n+            cr = ClassificationResult(\n+                cp[\"classification\"], cp[\"page\"], cp[\"prob\"], f\"0 Priority Label\"\n+            )\n+            return cr\n+\n     classified_pages = sorted(classified_pages, key=lambda x: x[\"label_priority\"])\n \n     high_priority_labels = ["
        },
        {
            "sha": "6aa5600fae1a6d6f3c3ea5a48af1c407c4b2e9c9",
            "message": "flach classifier",
            "code_changes": "@@ -4,9 +4,8 @@\n from authenticator import UserAuthentication\n from constants import CLASSIFICATION_MODELS\n from fellow2kv.extension import db\n-from helper.custom_classifier_v2.iclassifier_model_processor import (\n-    IClassifierModelProcessor,\n-)\n+from helper.custom_classifier_v2.iclassifier_model_processor import \\\n+    IClassifierModelProcessor\n from util import sync_wrapper\n \n from . import common, mlp_classifier_processor, service_classifier_processor\n@@ -189,7 +188,7 @@ def get_highest_priority_label(model_labels, result):\n             page[\"label_doc_type\"] = doc_type\n             page[\"label_priority\"] = priority\n \n-    # getting classification labels with priority > 0\n+    # getting classification labels with priority >= 0\n     result_pages_all = copy.deepcopy(result[\"pages\"])\n     result_pages_all = [cl for cl in result_pages_all if page[\"classification\"]]\n     classified_pages = [\n@@ -200,13 +199,22 @@ def get_highest_priority_label(model_labels, result):\n         return None\n \n     if not classified_pages:\n-        # first label returned\n+        # first label returned, if there is no entry for the classified label found\n         result_page = result_pages_all[0]\n         cr = ClassificationResult(\n-            result_page[\"classification\"], result_page, -1, f\"First label\"\n+            result_page[\"classification\"], result_page[\"page\"], -1, f\"First label\"\n         )\n         return cr\n \n+    # If there is any label with priority 0. It should be overall label for the document\n+    # irrespective of other labels and their scores.\n+    for cp in classified_pages:\n+        if cp.get(\"label_priority\", -1) == 0:\n+            cr = ClassificationResult(\n+                cp[\"classification\"], cp[\"page\"], cp[\"prob\"], f\"0 Priority Label\"\n+            )\n+            return cr\n+\n     classified_pages = sorted(classified_pages, key=lambda x: x[\"label_priority\"])\n \n     high_priority_labels = ["
        },
        {
            "sha": "4c584aa9d92ed06fcd005d9f6ea9b36015c9dfcf",
            "message": "Merge pull request #816 from Fellow-Consulting-AG/stage\n\nfuzzy primary field",
            "code_changes": "@@ -52,48 +52,42 @@ def verify_token(token):\n             res = UserAuthentication.verify_auth_token(token)\n             return res\n \n-        from namespaces.block_table_extract import (\n-            namespace as namespace_block_table_extract,\n-        )\n+        from namespaces.block_table_extract import \\\n+            namespace as namespace_block_table_extract\n         from namespaces.cache import namespace as namespace_cache\n-        from namespaces.custom_classifier import (\n-            namespace as namespace_custom_classifier,\n-        )\n-        from namespaces.custom_classifier_v2 import (\n-            namespace as namespace_custom_classifier_v2,\n-        )\n-        from namespaces.custom_models.custom_model import (\n-            namespace as namespace_custom_model_v2,\n-        )\n-        from namespaces.custom_models.custom_model_label import (\n-            namespace as namespace_custom_model_labels_v2,\n-        )\n-\n+        from namespaces.custom_classifier import \\\n+            namespace as namespace_custom_classifier\n+        from namespaces.custom_classifier_v2 import \\\n+            namespace as namespace_custom_classifier_v2\n+        from namespaces.custom_models.custom_model import \\\n+            namespace as namespace_custom_model_v2\n+        from namespaces.custom_models.custom_model_label import \\\n+            namespace as namespace_custom_model_labels_v2\n         # from namespaces.db_import_export import namespace as namespace_db\n         from namespaces.doc2_v2 import namespace as namespace_doc2_v2\n-        from namespaces.document_and_fields import (\n-            namespace as namespace_document_and_fields,\n-        )\n-        from namespaces.document_layout_template import (\n-            namespace as namespace_document_layout_template,\n-        )\n-        from namespaces.document_table import namespace as namespace_document_table\n+        from namespaces.document_and_fields import \\\n+            namespace as namespace_document_and_fields\n+        from namespaces.document_layout_template import \\\n+            namespace as namespace_document_layout_template\n+        from namespaces.document_table import \\\n+            namespace as namespace_document_table\n         from namespaces.export import namespace as namespace_export\n         from namespaces.extract import namespace as namespace_extract\n         from namespaces.health import namespace as namespace_health\n-        from namespaces.list_of_values.list_of_values import (\n-            namespace as namespace_list_of_values,\n-        )\n+        from namespaces.internal import namespace as namespace_internal\n+        from namespaces.list_of_values.list_of_values import \\\n+            namespace as namespace_list_of_values\n         from namespaces.migration import namespace as namespace_migration\n-        from namespaces.ocr_configurations import (\n-            namespace as namespace_ocr_configurations,\n-        )\n+        from namespaces.ocr_configurations import \\\n+            namespace as namespace_ocr_configurations\n         from namespaces.pdf import namespace as namespace_pdf\n         from namespaces.preferences import namespace as preferences\n-        from namespaces.sub_document_type import (\n-            namespace as namespace_sub_document_type,\n-        )\n-        from namespaces.table_extract_v3 import namespace as namespace_table_extract_v3\n+        from namespaces.sub_document_type import \\\n+            namespace as namespace_sub_document_type\n+        from namespaces.table_extract_v3 import \\\n+            namespace as namespace_table_extract_v3\n+\n+        \n \n         api.add_namespace(namespace_extract, path=\"/extract\")\n         api.add_namespace(namespace_export, path=\"/export\")\n@@ -124,6 +118,10 @@ def verify_token(token):\n         api.add_namespace(namespace_cache, path=\"/cache\")\n         api.add_namespace(namespace_health, path=\"/api\")\n \n+        if config.IS_DEBUG:\n+            api.add_namespace(namespace_internal, path=\"/internal\")\n+\n+\n         from fellow2kv.cli.ml import ml_blueprint\n \n         app.register_blueprint(ml_blueprint)"
        },
        {
            "sha": "7a5f04001fc7617ac0bb0cd110fcc800fff797bf",
            "message": "Merge branch 'sandbox' into stage",
            "code_changes": "@@ -7,16 +7,23 @@\n from typing import Any, Iterable\n \n from pdfminer.high_level import extract_pages\n-from pdfminer.layout import (LAParams, LTAnno, LTChar, LTFigure, LTPage,\n-                             LTTextBox, LTTextLine, LTTextLineHorizontal)\n+from pdfminer.layout import (\n+    LAParams,\n+    LTAnno,\n+    LTChar,\n+    LTFigure,\n+    LTPage,\n+    LTTextBox,\n+    LTTextLine,\n+    LTTextLineHorizontal,\n+)\n from shapely.geometry import Polygon\n \n from fellow2kv.extension import storagemanager_doc2 as storage_manager_doc2\n from helper.ai.ai_ocr import check_for_existing_line\n from helper.ai.google_ocr import GOOGLE_OCR\n from helper.extract_util import Extractor\n-from helper.table_extractors.table_formatter_v3 import \\\n-    get_intersection_area_percent\n+from helper.table_extractors.table_formatter_v3 import get_intersection_area_percent\n \n pattern = re.compile(\"^\\(cid:\\d+\\)+$\")\n "
        },
        {
            "sha": "f791b931df894d1031833442c7d06351b105bfff",
            "message": "Merge pull request #815 from Fellow-Consulting-AG/dev\n\nfuzzy primary field",
            "code_changes": "@@ -52,48 +52,42 @@ def verify_token(token):\n             res = UserAuthentication.verify_auth_token(token)\n             return res\n \n-        from namespaces.block_table_extract import (\n-            namespace as namespace_block_table_extract,\n-        )\n+        from namespaces.block_table_extract import \\\n+            namespace as namespace_block_table_extract\n         from namespaces.cache import namespace as namespace_cache\n-        from namespaces.custom_classifier import (\n-            namespace as namespace_custom_classifier,\n-        )\n-        from namespaces.custom_classifier_v2 import (\n-            namespace as namespace_custom_classifier_v2,\n-        )\n-        from namespaces.custom_models.custom_model import (\n-            namespace as namespace_custom_model_v2,\n-        )\n-        from namespaces.custom_models.custom_model_label import (\n-            namespace as namespace_custom_model_labels_v2,\n-        )\n-\n+        from namespaces.custom_classifier import \\\n+            namespace as namespace_custom_classifier\n+        from namespaces.custom_classifier_v2 import \\\n+            namespace as namespace_custom_classifier_v2\n+        from namespaces.custom_models.custom_model import \\\n+            namespace as namespace_custom_model_v2\n+        from namespaces.custom_models.custom_model_label import \\\n+            namespace as namespace_custom_model_labels_v2\n         # from namespaces.db_import_export import namespace as namespace_db\n         from namespaces.doc2_v2 import namespace as namespace_doc2_v2\n-        from namespaces.document_and_fields import (\n-            namespace as namespace_document_and_fields,\n-        )\n-        from namespaces.document_layout_template import (\n-            namespace as namespace_document_layout_template,\n-        )\n-        from namespaces.document_table import namespace as namespace_document_table\n+        from namespaces.document_and_fields import \\\n+            namespace as namespace_document_and_fields\n+        from namespaces.document_layout_template import \\\n+            namespace as namespace_document_layout_template\n+        from namespaces.document_table import \\\n+            namespace as namespace_document_table\n         from namespaces.export import namespace as namespace_export\n         from namespaces.extract import namespace as namespace_extract\n         from namespaces.health import namespace as namespace_health\n-        from namespaces.list_of_values.list_of_values import (\n-            namespace as namespace_list_of_values,\n-        )\n+        from namespaces.internal import namespace as namespace_internal\n+        from namespaces.list_of_values.list_of_values import \\\n+            namespace as namespace_list_of_values\n         from namespaces.migration import namespace as namespace_migration\n-        from namespaces.ocr_configurations import (\n-            namespace as namespace_ocr_configurations,\n-        )\n+        from namespaces.ocr_configurations import \\\n+            namespace as namespace_ocr_configurations\n         from namespaces.pdf import namespace as namespace_pdf\n         from namespaces.preferences import namespace as preferences\n-        from namespaces.sub_document_type import (\n-            namespace as namespace_sub_document_type,\n-        )\n-        from namespaces.table_extract_v3 import namespace as namespace_table_extract_v3\n+        from namespaces.sub_document_type import \\\n+            namespace as namespace_sub_document_type\n+        from namespaces.table_extract_v3 import \\\n+            namespace as namespace_table_extract_v3\n+\n+        \n \n         api.add_namespace(namespace_extract, path=\"/extract\")\n         api.add_namespace(namespace_export, path=\"/export\")\n@@ -124,6 +118,10 @@ def verify_token(token):\n         api.add_namespace(namespace_cache, path=\"/cache\")\n         api.add_namespace(namespace_health, path=\"/api\")\n \n+        if config.IS_DEBUG:\n+            api.add_namespace(namespace_internal, path=\"/internal\")\n+\n+\n         from fellow2kv.cli.ml import ml_blueprint\n \n         app.register_blueprint(ml_blueprint)"
        },
        {
            "sha": "41cdeb11266b6f0940ede751bad0f97fd5707801",
            "message": "fuzzy primary field",
            "code_changes": "@@ -62,6 +62,7 @@ def get_document_tables(org_id, doc_type):\n     \"calculation_formula\",\n     \"fuzzy_name\",\n     \"fuzzy_field\",\n+    \"fuzzy_primary_field\",\n     \"is_sub_doc_type_field\",\n ]\n \n@@ -80,6 +81,7 @@ def get_document_tables(org_id, doc_type):\n     \"sort_order\",\n     \"fuzzy_name\",\n     \"fuzzy_field\",\n+    \"fuzzy_primary_field\",\n     \"is_required\",\n     \"is_readonly\",\n     \"force_validation\",\n@@ -194,7 +196,7 @@ def get_field_validation_rules_compact(db, doc_type, sub_doc_type, profile, org_\n     query = f\"\"\"select\n         fg.id as group_id, fg.group_name as group_name, fg.title as group_title, fg.translation_key as group_translation_key,\n         f.id, f.org_id as field_org_id, f.doc_type, f.field_name, f.title, f.translation_key, f.calculation_formula,\n-        f.field_type,f.fuzzy_field, f.fuzzy_name, f.is_sub_doc_type_field,\n+        f.field_type,f.fuzzy_field, f.fuzzy_name,fuzzy_primary_field, f.is_sub_doc_type_field,\n         case\n             when fvr.id is not null then fvr.is_required\n             else dfvr.is_required\n@@ -602,7 +604,7 @@ def get_table_column_confs(org_id: str, table_name: str, column_name=None):\n \n def get_fuzzy_fields_list(db, org_id, doc_type, sub_doc_type):\n     query = f\"\"\"select\n-        f.doc_type, f.field_name, fuzzy_name, f.fuzzy_field\n+        f.doc_type, f.field_name, fuzzy_name, f.fuzzy_field, f.fuzzy_primary_field,\n     from\n         field f\n     Where ((f.doc_type in ('{doc_type}') and f.is_sub_doc_type_field = False)"
        },
        {
            "sha": "082dd8b50f80dc9810bf4b3d4028ee9a89911320",
            "message": "internal endpoint added",
            "code_changes": "@@ -52,48 +52,42 @@ def verify_token(token):\n             res = UserAuthentication.verify_auth_token(token)\n             return res\n \n-        from namespaces.block_table_extract import (\n-            namespace as namespace_block_table_extract,\n-        )\n+        from namespaces.block_table_extract import \\\n+            namespace as namespace_block_table_extract\n         from namespaces.cache import namespace as namespace_cache\n-        from namespaces.custom_classifier import (\n-            namespace as namespace_custom_classifier,\n-        )\n-        from namespaces.custom_classifier_v2 import (\n-            namespace as namespace_custom_classifier_v2,\n-        )\n-        from namespaces.custom_models.custom_model import (\n-            namespace as namespace_custom_model_v2,\n-        )\n-        from namespaces.custom_models.custom_model_label import (\n-            namespace as namespace_custom_model_labels_v2,\n-        )\n-\n+        from namespaces.custom_classifier import \\\n+            namespace as namespace_custom_classifier\n+        from namespaces.custom_classifier_v2 import \\\n+            namespace as namespace_custom_classifier_v2\n+        from namespaces.custom_models.custom_model import \\\n+            namespace as namespace_custom_model_v2\n+        from namespaces.custom_models.custom_model_label import \\\n+            namespace as namespace_custom_model_labels_v2\n         # from namespaces.db_import_export import namespace as namespace_db\n         from namespaces.doc2_v2 import namespace as namespace_doc2_v2\n-        from namespaces.document_and_fields import (\n-            namespace as namespace_document_and_fields,\n-        )\n-        from namespaces.document_layout_template import (\n-            namespace as namespace_document_layout_template,\n-        )\n-        from namespaces.document_table import namespace as namespace_document_table\n+        from namespaces.document_and_fields import \\\n+            namespace as namespace_document_and_fields\n+        from namespaces.document_layout_template import \\\n+            namespace as namespace_document_layout_template\n+        from namespaces.document_table import \\\n+            namespace as namespace_document_table\n         from namespaces.export import namespace as namespace_export\n         from namespaces.extract import namespace as namespace_extract\n         from namespaces.health import namespace as namespace_health\n-        from namespaces.list_of_values.list_of_values import (\n-            namespace as namespace_list_of_values,\n-        )\n+        from namespaces.internal import namespace as namespace_internal\n+        from namespaces.list_of_values.list_of_values import \\\n+            namespace as namespace_list_of_values\n         from namespaces.migration import namespace as namespace_migration\n-        from namespaces.ocr_configurations import (\n-            namespace as namespace_ocr_configurations,\n-        )\n+        from namespaces.ocr_configurations import \\\n+            namespace as namespace_ocr_configurations\n         from namespaces.pdf import namespace as namespace_pdf\n         from namespaces.preferences import namespace as preferences\n-        from namespaces.sub_document_type import (\n-            namespace as namespace_sub_document_type,\n-        )\n-        from namespaces.table_extract_v3 import namespace as namespace_table_extract_v3\n+        from namespaces.sub_document_type import \\\n+            namespace as namespace_sub_document_type\n+        from namespaces.table_extract_v3 import \\\n+            namespace as namespace_table_extract_v3\n+\n+        \n \n         api.add_namespace(namespace_extract, path=\"/extract\")\n         api.add_namespace(namespace_export, path=\"/export\")\n@@ -124,6 +118,10 @@ def verify_token(token):\n         api.add_namespace(namespace_cache, path=\"/cache\")\n         api.add_namespace(namespace_health, path=\"/api\")\n \n+        if config.IS_DEBUG:\n+            api.add_namespace(namespace_internal, path=\"/internal\")\n+\n+\n         from fellow2kv.cli.ml import ml_blueprint\n \n         app.register_blueprint(ml_blueprint)"
        },
        {
            "sha": "6ce930425f8ed6b6e2b00e3fc0fe292d1291da4d",
            "message": "db upgrade",
            "code_changes": "@@ -0,0 +1,28 @@\n+\"\"\"fuzzy_primary_field column added\n+\n+Revision ID: 94e7ca15c0d3\n+Revises: 777bef049c09\n+Create Date: 2023-03-14 10:28:39.029274\n+\n+\"\"\"\n+from alembic import op\n+import sqlalchemy as sa\n+\n+\n+# revision identifiers, used by Alembic.\n+revision = '94e7ca15c0d3'\n+down_revision = '777bef049c09'\n+branch_labels = None\n+depends_on = None\n+\n+\n+def upgrade():\n+    # ### commands auto generated by Alembic - please adjust! ###\n+    op.add_column('field', sa.Column('fuzzy_primary_field', sa.Boolean(), server_default='f', nullable=True))\n+    # ### end Alembic commands ###\n+\n+\n+def downgrade():\n+    # ### commands auto generated by Alembic - please adjust! ###\n+    op.drop_column('field', 'fuzzy_primary_field')\n+    # ### end Alembic commands ###"
        }
    ]
}